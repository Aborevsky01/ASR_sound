# ASR_sound

## Installation guide

Upload github repository to local server:
```shell
!git clone https://github.com/Aborevsky01/ASR_sound.git
 ```

Install required libraries (might be minor issues not affecting the following procedure)
```shell
!pip install -r ASR_sound/requirements.txt
 ```
 
Run best model
```shell
%run -i 'train.py'  -c hw_asr/default_test_model/config.json -r hw_asr/default_test_model/checkpoint.pth
 ```
 
 Download model's checkpoint 
 
 ```shell
 !wget https://www.dropbox.com/s/1jakftswt90udtu/checkpoint.pth?dl=0 -o hw_asr/default_test_model/checkpoint.pth
 ```
 
 Run test
 
 ```shell
python test.py \
   -c default_test_config.json \
   -r default_test_model/checkpoint.pth \
   -o test_result.json
 ```
 
 
 Download language models:
 
 1. 4gram kenlm() - скачивание происходит из https://openslr.elda.org/resources/11/4-gram.arpa.gz и осуществляется внутри функции BPE_models.kenlm_path()
 
 2. pretrained gpt2 - скачивание происходит внутри lm_scorer.models.auto.LMScorer.from_pretrained("gpt2")
 
 
 ## Directories description
 
 1. `Lm_scorer` - это общедоступная одноименная библиотека для создания pretrained rescorer LM. Ее действительно можно скачать напрямую через терминал
 без создания конкретной директории, однако в ходе работы было принято именно такое решение.
 
 2. `BPE_models` - папка, включающая в себя дополнительные объекты, оказавшиеся своего рода надбавкой над основными задачами. lowercase.arpa представляет 
 из себя преобученный 4-gram kenlm, получаемый благодаря функции `kenlm_path()`. 
 Различного вида словари, создаваемые функцией `bpe_train`, сохраняются так же в эту папку (см. BPE в config).
 

## Brief report  

* How to reproduce your model?

(0. 20 epochs on single batch, no Aug, with BPE on dev-clean, with config one_batch_test.json) not mandatory

1. 3 epochs on train-100, no Aug, with BPE on train-100, with config `hundred_librispeech.json`

2. 3 epochs on train-100, with Aug, with BPE on train-100, with config `hundred_aug_librispeech.json` (in current default model - 2 epochs)

3. 6 epochs on train-360, with Aug, with BPE on train-100, with config `threesixty_librispeech.json`

* How did you train your final model?

Финальная модель обучалась в духе рекоммендаций: волновым образом. То есть, сначала выбранная модель была переобучена на одном батче.
Далее она в так называемом pretraining phase работала с train-100 датасетом без каких либо аугментаций, но с BPE в течение 3 эпох. На этой стадии были
минимизрованы сторонние вычисления с целью быстрого получения хорошей модели. Далее было еще 3 эпохи на train-100 до достижения определенных benchmarks,
в том числе благодаря привнесенным изменениям как касательно бим-серча, так и агументаций. На последней же стадии модель работала с большим датасетом 
train-360 в течение пяти эпох. Конечно, данный подход можно было бы продлить вплоть до CommonVoice и так далее, однако временные ограничения не позволяют 
сделать этого на данный момент. Так же добавим, что своего рода волновой подход с запуском новых эпох не последовательно, а с обнулением и загрузкой чекпоинта позволяет немного взорвать подзатухающие веса. давая тем самым дополнительный прирост по качеству.

* What have you tried?

Преимущественно обращал внимание на модели, аналогичные DeepSpeech, ввиду их наибольшей понятности, интерпретируемости, большому числу реализаций и 
легкостью в трансформации. Пробовались различные комбинации гиперпараметров, намеренно чрезмерно глубокие / неглубокие сетки, кардинально низкие / высокие 
`learning rate max` для `schedruler`, разнообразные функции активации и чрезмерно большие значения `dropout`. 

Однако, конечно, нельзя не отмтеить ни внимания и попыток реализаций как других статей из предложенных, так и совершенно изохищренных методов, вроде lbfgs. Особо много времени занял подбор предобученной n-gram kenlm, так как первоначально выбранные версии не отличались высоким качеством.

* What worked and what didn't work?

1. worked

>Очень спасали residual connections, которые позволяли проталкивать градиент дальше к началу модели, тем самым не позволяя последним затухаит.
>
> Действительно хорошо улучшали качество обе LM и BPE.

2. didn't work

> Изначально мешали аугментации, при использовании которых на самом старте, модель ударялась в тупик. Более того, крайне важным оказался параметр 
вероятности, которая применяемым агументациям ставится.
>
> Было достаточно опреметчивым решением стараться взять числом слоев. При этом обращая внимание на статьи, предоставленные для изучения, стало понятно 
примерно оптимальное число слоев каждого вида. При этом очень много времени занял подбор lr, при определенных значения выводивший всю модель из строя.
Также отметим важность выбора оптимизатора, ведь при попытке замены AdamW performance модели оказывался слабее.

3. Был включен датасет LJ, который советовали использовать, однако он не только не дал ощутимой разницы в последующей работе модели, но и занял достаточно
серьезное время разрещение реализационных багов.

* What were the major challenges?

> *Ниже привожу свои мысли, когда я еще не знал, что в датасфере можно пользоваться GPU:*
>
> Одной из наибольшщих сложностей оказалась именно вычислительная сторона работы.
К примеру, за разумное время было практически невозможно обучить модель на train-360, так датасфера показывала крайне низкую производительность, 
а возможности google colab ограничены (если не брать коммерческие планы). Поэтому есть ощущение, что при больших производительностях, 
доступных для выполнения работы, результаты оказались бы более высокими.
>
> Хочется оттметить, что на сохранение данных частично повлиял google colab, используемый как источник GPU (на DL GPU в датасфере пользоваться нельзя), который выбрасывал все данные из системы в момент их сохранения. В связи с этим на эпохах 2-3 значение steps слегка переехали, однако это никоим образом не влияент на работу модели. Также по непонятным причинам при запуске валидации colab опрокидывал работу ячеек, что заставляло запускать эпохи с чекпоинтов. Благо в конце стала ясна моя ошибка и в датасфере удалось наладить работу модели.

> Также отметим определенные проблемы, которые возникали при синхронизации словарей объекта внешнего класса `BeamSearchDecoder `
и реализованного нами `CTCCharTextEncoder`. Определеенные сложности чинила BPE, вынуждавшая обращать внимание на соответствие токенов индексам и наоборот.

> Некоторое время пришлось потратить на борьбу с небезызвествным порогом CTCloss ~ 3. Некоторые мои модели не могли обойти данный барьер, что
вынуждало искать слабые места и более внимательно относиться к градиентам.

## Accomplished bonuses

> Была реализована языковая модель kenlm, работающая на словаре n-gram (4 в нашем случае) в качестве техники shallow fusion. Примняется внутри корпуса 
алгоритма beam-search из библиотеки pyctcdecode, уже упоминаемой ранее.

> Была реализована языковая модель LM_model, работающая на основе pretrained gpt-2 в качестве техники rescorer. Примняется после работы 
beam_search + shallow fusion для дальнейшей оценки гипотез.

> Был реализован алгоритм BPE, создающий библиотеку из токенов на основе их частоты во входном тексте.
