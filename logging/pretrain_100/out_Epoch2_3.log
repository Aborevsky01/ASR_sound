Loading checkpoint: /content/saved/models/default_config/1017_022955/model_best.pth ...
Checkpoint loaded. Resume training from epoch 2
['', 'at', 'as', 't', 'ing', 'l', '_', '‚ñÅb', 'er', '‚ñÅn', 'd', 'q', 'k', 'h', '‚ñÅto', 'he', 'p', '‚ñÅt', '‚ñÅth', '‚ñÅof', 'y', '‚ñÅthe', '‚ñÅ', 'b', 'v', 'en', '‚ñÅs', 're', '‚ñÅm', 'in', 'u', 'es', '‚ñÅw', 'z', 'w', 'e', 'f', '‚ñÅo', '‚ñÅh', '‚ñÅi', '‚ñÅf', 'on', 'r', 'ou', '‚ñÅand', 'j', '‚ñÅa', 'ar', 'i', 'ed', 'o', 'it', '</s>', 'c', 'is', 'n', 'g', 'or', '‚ñÅin', 'x', '<s>', 'a', 'nd', '‚ñÅc', 's', '‚ñÅhe', 'an', '‚ñÅd', 'm', 'll', '<unk>', '‚ñÅp', '‚ñÅl']
Train Epoch: 2 [0/2853 (0%)] Loss: 13.771022




































train:   4%|‚ñé         | 100/2853 [01:14<33:59,  1.35it/s]





































train:   7%|‚ñã         | 200/2853 [02:31<32:44,  1.35it/s]




































train:  11%|‚ñà         | 300/2853 [03:45<31:51,  1.34it/s]





































train:  14%|‚ñà‚ñç        | 399/2853 [05:01<31:42,  1.29it/s]





































train:  18%|‚ñà‚ñä        | 500/2853 [06:17<29:17,  1.34it/s]





































train:  21%|‚ñà‚ñà        | 600/2853 [07:33<27:18,  1.38it/s]



































train:  25%|‚ñà‚ñà‚ñç       | 700/2853 [08:47<26:33,  1.35it/s]





































train:  28%|‚ñà‚ñà‚ñä       | 800/2853 [10:03<25:42,  1.33it/s]




































train:  32%|‚ñà‚ñà‚ñà‚ñè      | 900/2853 [11:18<24:10,  1.35it/s]





































train:  35%|‚ñà‚ñà‚ñà‚ñå      | 1000/2853 [12:33<23:49,  1.30it/s]





































train:  39%|‚ñà‚ñà‚ñà‚ñä      | 1100/2853 [13:49<21:10,  1.38it/s]




































train:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1200/2853 [15:04<21:17,  1.29it/s]




































train:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1300/2853 [16:20<19:10,  1.35it/s]





































train:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1400/2853 [17:36<17:42,  1.37it/s]





































train:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1500/2853 [18:52<17:56,  1.26it/s]





































train:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1600/2853 [20:08<16:05,  1.30it/s]





































train:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1700/2853 [21:23<14:22,  1.34it/s]




































train:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1800/2853 [22:39<13:09,  1.33it/s]





































train:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 1900/2853 [23:53<11:51,  1.34it/s]





































train:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2000/2853 [25:08<10:25,  1.36it/s]





































train:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2100/2853 [26:23<09:39,  1.30it/s]





































train:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2200/2853 [27:37<08:11,  1.33it/s]





































train:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2300/2853 [28:52<06:42,  1.37it/s]





































train:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2400/2853 [30:08<05:28,  1.38it/s]





































train:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2500/2853 [31:24<04:36,  1.28it/s]




































train:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2600/2853 [32:39<03:14,  1.30it/s]





































train:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 2700/2853 [33:54<01:53,  1.34it/s]





































train:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 2800/2853 [35:11<00:40,  1.31it/s]


















train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2853/2853 [35:51<00:00,  1.33it/s]








































val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [01:50<00:00,  2.08it/s]
    epoch          : 2
    loss           : 1.0339849787950515
    grad norm      : 1.7142660534381866
    WER (ARG)      : 0.7730113355390646
    CER (ARG)      : 0.4696989973258908
    CER (BMS)      : 0
    WER (BMS)      : 0
    val_loss       : 1.0679793831562892
    val_WER (ARG)  : 0.7751736743884147
    val_CER (ARG)  : 0.46485381766257927
    val_CER (BMS)  : 1.1619376731062523
    val_WER (BMS)  : 0.9981729222191156
Saving current best: model_best.pth ...
train:   0%|          | 0/2853 [00:00<?, ?it/s]





































train:   4%|‚ñé         | 100/2853 [01:15<33:59,  1.35it/s]





































train:   7%|‚ñã         | 200/2853 [02:31<33:34,  1.32it/s]




































train:  11%|‚ñà         | 300/2853 [03:46<31:36,  1.35it/s]





































train:  14%|‚ñà‚ñç        | 400/2853 [05:01<30:27,  1.34it/s]




































train:  18%|‚ñà‚ñä        | 500/2853 [06:16<29:46,  1.32it/s]






































train:  21%|‚ñà‚ñà        | 600/2853 [07:33<28:04,  1.34it/s]





































train:  25%|‚ñà‚ñà‚ñç       | 700/2853 [08:48<27:26,  1.31it/s]






































train:  28%|‚ñà‚ñà‚ñä       | 800/2853 [10:05<25:33,  1.34it/s]





































train:  32%|‚ñà‚ñà‚ñà‚ñè      | 900/2853 [11:21<24:46,  1.31it/s]





































train:  35%|‚ñà‚ñà‚ñà‚ñå      | 1000/2853 [12:37<23:31,  1.31it/s]





































train:  39%|‚ñà‚ñà‚ñà‚ñä      | 1100/2853 [13:54<22:07,  1.32it/s]





































train:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1200/2853 [15:09<19:38,  1.40it/s]





































train:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1300/2853 [16:25<19:23,  1.33it/s]




































train:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1400/2853 [17:40<17:44,  1.37it/s]





































train:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1500/2853 [18:56<16:19,  1.38it/s]





































train:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1600/2853 [20:12<15:19,  1.36it/s]





































train:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1700/2853 [21:28<14:26,  1.33it/s]




































train:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1800/2853 [22:43<13:24,  1.31it/s]





































train:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 1900/2853 [23:59<11:45,  1.35it/s]





































train:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2000/2853 [25:15<10:34,  1.34it/s]





































train:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2100/2853 [26:30<09:16,  1.35it/s]





































train:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2200/2853 [27:47<08:28,  1.28it/s]




































train:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2300/2853 [29:01<06:29,  1.42it/s]





































train:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2400/2853 [30:17<05:37,  1.34it/s]




































train:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2500/2853 [31:31<04:20,  1.35it/s]





































train:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2600/2853 [32:47<03:12,  1.32it/s]




































train:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 2700/2853 [34:01<01:52,  1.36it/s]





































train:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 2800/2853 [35:16<00:40,  1.32it/s]


















train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2853/2853 [35:57<00:00,  1.32it/s]






































val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 229/229 [01:44<00:00,  2.20it/s]
    epoch          : 3
    loss           : 0.8249049735069275
    grad norm      : 1.6075188601016999
    WER (ARG)      : 0.6825036009087926
    CER (ARG)      : 0.41632757972337786
    CER (BMS)      : 0
    WER (BMS)      : 0
    val_loss       : 0.8776273985096461
    val_WER (ARG)  : 0.7039567875964392
    val_CER (ARG)  : 0.4215237589253938
    val_CER (BMS)  : 1.3380705317788686
    val_WER (BMS)  : 1.0010127654027026
Saving current best: model_best.pth ...
Using arpa instead of binary LM file, decoder instantiation might be slow.
Alphabet determined to be of BPE style.
CTC blank char '' not found, appending to end.
Found <unk> in vocabulary, substituting with ‚ñÅ‚Åá‚ñÅ.
92 (3.5%) records are longer then 20.0 seconds. Excluding them.
334 (12.7%) records are longer then 200 characters. Excluding them.
Filtered 335(12.8%) records  from dataset
TrialModel(
  (cnn): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (resconv): Sequential(
    (0): ConvBlock(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (layer_norm1): Normalize()
      (layer_norm2): Normalize()
    )
    (1): ConvBlock(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (layer_norm1): Normalize()
      (layer_norm2): Normalize()
    )
    (2): ConvBlock(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (layer_norm1): Normalize()
      (layer_norm2): Normalize()
    )
  )
  (fc_1): Linear(in_features=2048, out_features=512, bias=True)
  (rnn): Sequential(
    (0): RNNBlock(
      (seq): Sequential(
        (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (1): ReLU6()
        (2): GRU(512, 512, batch_first=True, bidirectional=True)
      )
    )
    (1): RNNBlock(
      (seq): Sequential(
        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (1): ReLU6()
        (2): GRU(1024, 512, bidirectional=True)
      )
    )
    (2): RNNBlock(
      (seq): Sequential(
        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (1): ReLU6()
        (2): GRU(1024, 512, bidirectional=True)
      )
    )
    (3): RNNBlock(
      (seq): Sequential(
        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (1): ReLU6()
        (2): GRU(1024, 512, bidirectional=True)
      )
    )
    (4): RNNBlock(
      (seq): Sequential(
        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (1): ReLU6()
        (2): GRU(1024, 512, bidirectional=True)
      )
    )
  )
  (fc_2): Linear(in_features=1024, out_features=512, bias=True)
  (out): Sequential(
    (0): Dropout(p=0.2, inplace=False)
    (1): ReLU6()
    (2): Linear(in_features=512, out_features=73, bias=True)
  )
)
Trainable parameters: 23727177
[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.login() after wandb.init() has no effect.