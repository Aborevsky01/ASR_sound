Loading checkpoint: /content/saved/models/default_config/1017_022955/model_best.pth ...
Checkpoint loaded. Resume training from epoch 2
['', 'at', 'as', 't', 'ing', 'l', '_', '▁b', 'er', '▁n', 'd', 'q', 'k', 'h', '▁to', 'he', 'p', '▁t', '▁th', '▁of', 'y', '▁the', '▁', 'b', 'v', 'en', '▁s', 're', '▁m', 'in', 'u', 'es', '▁w', 'z', 'w', 'e', 'f', '▁o', '▁h', '▁i', '▁f', 'on', 'r', 'ou', '▁and', 'j', '▁a', 'ar', 'i', 'ed', 'o', 'it', '</s>', 'c', 'is', 'n', 'g', 'or', '▁in', 'x', '<s>', 'a', 'nd', '▁c', 's', '▁he', 'an', '▁d', 'm', 'll', '<unk>', '▁p', '▁l']
Train Epoch: 2 [0/2853 (0%)] Loss: 13.771022




































train:   4%|▎         | 100/2853 [01:14<33:59,  1.35it/s]





































train:   7%|▋         | 200/2853 [02:31<32:44,  1.35it/s]




































train:  11%|█         | 300/2853 [03:45<31:51,  1.34it/s]





































train:  14%|█▍        | 399/2853 [05:01<31:42,  1.29it/s]





































train:  18%|█▊        | 500/2853 [06:17<29:17,  1.34it/s]





































train:  21%|██        | 600/2853 [07:33<27:18,  1.38it/s]



































train:  25%|██▍       | 700/2853 [08:47<26:33,  1.35it/s]





































train:  28%|██▊       | 800/2853 [10:03<25:42,  1.33it/s]




































train:  32%|███▏      | 900/2853 [11:18<24:10,  1.35it/s]





































train:  35%|███▌      | 1000/2853 [12:33<23:49,  1.30it/s]





































train:  39%|███▊      | 1100/2853 [13:49<21:10,  1.38it/s]




































train:  42%|████▏     | 1200/2853 [15:04<21:17,  1.29it/s]




































train:  46%|████▌     | 1300/2853 [16:20<19:10,  1.35it/s]





































train:  49%|████▉     | 1400/2853 [17:36<17:42,  1.37it/s]





































train:  53%|█████▎    | 1500/2853 [18:52<17:56,  1.26it/s]





































train:  56%|█████▌    | 1600/2853 [20:08<16:05,  1.30it/s]





































train:  60%|█████▉    | 1700/2853 [21:23<14:22,  1.34it/s]




































train:  63%|██████▎   | 1800/2853 [22:39<13:09,  1.33it/s]





































train:  67%|██████▋   | 1900/2853 [23:53<11:51,  1.34it/s]





































train:  70%|███████   | 2000/2853 [25:08<10:25,  1.36it/s]





































train:  74%|███████▎  | 2100/2853 [26:23<09:39,  1.30it/s]





































train:  77%|███████▋  | 2200/2853 [27:37<08:11,  1.33it/s]





































train:  81%|████████  | 2300/2853 [28:52<06:42,  1.37it/s]





































train:  84%|████████▍ | 2400/2853 [30:08<05:28,  1.38it/s]





































train:  88%|████████▊ | 2500/2853 [31:24<04:36,  1.28it/s]




































train:  91%|█████████ | 2600/2853 [32:39<03:14,  1.30it/s]





































train:  95%|█████████▍| 2700/2853 [33:54<01:53,  1.34it/s]





































train:  98%|█████████▊| 2800/2853 [35:11<00:40,  1.31it/s]


















train: 100%|██████████| 2853/2853 [35:51<00:00,  1.33it/s]








































val: 100%|██████████| 229/229 [01:50<00:00,  2.08it/s]
    epoch          : 2
    loss           : 1.0339849787950515
    grad norm      : 1.7142660534381866
    WER (ARG)      : 0.7730113355390646
    CER (ARG)      : 0.4696989973258908
    CER (BMS)      : 0
    WER (BMS)      : 0
    val_loss       : 1.0679793831562892
    val_WER (ARG)  : 0.7751736743884147
    val_CER (ARG)  : 0.46485381766257927
    val_CER (BMS)  : 1.1619376731062523
    val_WER (BMS)  : 0.9981729222191156
Saving current best: model_best.pth ...
train:   0%|          | 0/2853 [00:00<?, ?it/s]





































train:   4%|▎         | 100/2853 [01:15<33:59,  1.35it/s]





































train:   7%|▋         | 200/2853 [02:31<33:34,  1.32it/s]




































train:  11%|█         | 300/2853 [03:46<31:36,  1.35it/s]





































train:  14%|█▍        | 400/2853 [05:01<30:27,  1.34it/s]




































train:  18%|█▊        | 500/2853 [06:16<29:46,  1.32it/s]






































train:  21%|██        | 600/2853 [07:33<28:04,  1.34it/s]





































train:  25%|██▍       | 700/2853 [08:48<27:26,  1.31it/s]






































train:  28%|██▊       | 800/2853 [10:05<25:33,  1.34it/s]





































train:  32%|███▏      | 900/2853 [11:21<24:46,  1.31it/s]





































train:  35%|███▌      | 1000/2853 [12:37<23:31,  1.31it/s]





































train:  39%|███▊      | 1100/2853 [13:54<22:07,  1.32it/s]





































train:  42%|████▏     | 1200/2853 [15:09<19:38,  1.40it/s]





































train:  46%|████▌     | 1300/2853 [16:25<19:23,  1.33it/s]




































train:  49%|████▉     | 1400/2853 [17:40<17:44,  1.37it/s]





































train:  53%|█████▎    | 1500/2853 [18:56<16:19,  1.38it/s]





































train:  56%|█████▌    | 1600/2853 [20:12<15:19,  1.36it/s]





































train:  60%|█████▉    | 1700/2853 [21:28<14:26,  1.33it/s]




































train:  63%|██████▎   | 1800/2853 [22:43<13:24,  1.31it/s]





































train:  67%|██████▋   | 1900/2853 [23:59<11:45,  1.35it/s]





































train:  70%|███████   | 2000/2853 [25:15<10:34,  1.34it/s]





































train:  74%|███████▎  | 2100/2853 [26:30<09:16,  1.35it/s]





































train:  77%|███████▋  | 2200/2853 [27:47<08:28,  1.28it/s]




































train:  81%|████████  | 2300/2853 [29:01<06:29,  1.42it/s]





































train:  84%|████████▍ | 2400/2853 [30:17<05:37,  1.34it/s]




































train:  88%|████████▊ | 2500/2853 [31:31<04:20,  1.35it/s]





































train:  91%|█████████ | 2600/2853 [32:47<03:12,  1.32it/s]




































train:  95%|█████████▍| 2700/2853 [34:01<01:52,  1.36it/s]





































train:  98%|█████████▊| 2800/2853 [35:16<00:40,  1.32it/s]


















train: 100%|██████████| 2853/2853 [35:57<00:00,  1.32it/s]






































val: 100%|██████████| 229/229 [01:44<00:00,  2.20it/s]
    epoch          : 3
    loss           : 0.8249049735069275
    grad norm      : 1.6075188601016999
    WER (ARG)      : 0.6825036009087926
    CER (ARG)      : 0.41632757972337786
    CER (BMS)      : 0
    WER (BMS)      : 0
    val_loss       : 0.8776273985096461
    val_WER (ARG)  : 0.7039567875964392
    val_CER (ARG)  : 0.4215237589253938
    val_CER (BMS)  : 1.3380705317788686
    val_WER (BMS)  : 1.0010127654027026
Saving current best: model_best.pth ...
Using arpa instead of binary LM file, decoder instantiation might be slow.
Alphabet determined to be of BPE style.
CTC blank char '' not found, appending to end.
Found <unk> in vocabulary, substituting with ▁⁇▁.
92 (3.5%) records are longer then 20.0 seconds. Excluding them.
334 (12.7%) records are longer then 200 characters. Excluding them.
Filtered 335(12.8%) records  from dataset
TrialModel(
  (cnn): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (resconv): Sequential(
    (0): ConvBlock(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (layer_norm1): Normalize()
      (layer_norm2): Normalize()
    )
    (1): ConvBlock(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (layer_norm1): Normalize()
      (layer_norm2): Normalize()
    )
    (2): ConvBlock(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (layer_norm1): Normalize()
      (layer_norm2): Normalize()
    )
  )
  (fc_1): Linear(in_features=2048, out_features=512, bias=True)
  (rnn): Sequential(
    (0): RNNBlock(
      (seq): Sequential(
        (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (1): ReLU6()
        (2): GRU(512, 512, batch_first=True, bidirectional=True)
      )
    )
    (1): RNNBlock(
      (seq): Sequential(
        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (1): ReLU6()
        (2): GRU(1024, 512, bidirectional=True)
      )
    )
    (2): RNNBlock(
      (seq): Sequential(
        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (1): ReLU6()
        (2): GRU(1024, 512, bidirectional=True)
      )
    )
    (3): RNNBlock(
      (seq): Sequential(
        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (1): ReLU6()
        (2): GRU(1024, 512, bidirectional=True)
      )
    )
    (4): RNNBlock(
      (seq): Sequential(
        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (1): ReLU6()
        (2): GRU(1024, 512, bidirectional=True)
      )
    )
  )
  (fc_2): Linear(in_features=1024, out_features=512, bias=True)
  (out): Sequential(
    (0): Dropout(p=0.2, inplace=False)
    (1): ReLU6()
    (2): Linear(in_features=512, out_features=73, bias=True)
  )
)
Trainable parameters: 23727177
[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.login() after wandb.init() has no effect.